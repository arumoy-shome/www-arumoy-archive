<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2021-01-30 Sat 20:45 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Km3NeT Log</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Arumoy Shome" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" href="assets/css/main.css" type="text/css"/>
<script type="text/javascript">
// @license magnet:?xt=urn:btih:1f739d935676111cfff4b4693e3816e664797050&amp;dn=gpl-3.0.txt GPL-v3-or-Later
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.cacheClassElem = elem.className;
         elem.cacheClassTarget = target.className;
         target.className = "code-highlighted";
         elem.className   = "code-highlighted";
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(elem.cacheClassElem)
         elem.className = elem.cacheClassElem;
       if(elem.cacheClassTarget)
         target.className = elem.cacheClassTarget;
     }
    /*]]>*///-->
// @license-end
</script>
</head>
<body>
<div id="content">
<h1 class="title">Km3NeT Log</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#orgb3f528b">Inbox</a>
<ul>
<li><a href="#org67a6e0f">directions for speedup</a></li>
<li><a href="#org5659a5b">[paper] recommendations to improve model</a></li>
</ul>
</li>
<li><a href="#org8b7eeee">Pattern Matrix</a>
<ul>
<li><a href="#orgda92555">Approach</a>
<ul>
<li><a href="#org361b5ab">MLP to predict <i>causally related</i> points</a></li>
</ul>
</li>
<li><a href="#org16a8c61">Creation of "Pattern Matrix" Dataset</a></li>
<li><a href="#orgfb99394">Evaluation</a>
<ul>
<li><a href="#org29aa802">Testing datasets</a></li>
</ul>
</li>
<li><a href="#org2ebc555"><span class="done DONE">DONE</span> Experiments</a>
<ul>
<li><a href="#org9e56e08">Notes on selection of epochs</a></li>
<li><a href="#org4e1aefd"><span class="done DONE">DONE</span> Experiments with dataset</a></li>
<li><a href="#orgbfe98cb"><span class="done DONE">DONE</span> Experiments with optimizers</a></li>
<li><a href="#org96516a6"><span class="done DONE">DONE</span> Experiments with model architecture</a></li>
</ul>
</li>
<li><a href="#orgfea82ee">Future Work</a>
<ul>
<li><a href="#org17073ee">Advanced relations</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgdad1005">Graph Community Detection</a>
<ul>
<li><a href="#orgd1ee1f7">Edge weights in GNNs</a></li>
<li><a href="#org0a426f9"><span class="done CANCELLED">CANCELLED</span> Why we do not need graphConv</a></li>
<li><a href="#orge603ac8">Primer on Graph Neural Networks</a></li>
<li><a href="#org860840f">Approach</a></li>
<li><a href="#org0038ecc">Data Preparation</a></li>
<li><a href="#orgd19cb7a">Evaluation</a></li>
<li><a href="#orgb361d64">Experiments</a>
<ul>
<li><a href="#org693718b">No edge features</a></li>
<li><a href="#org57edca1">Scalar edge features</a></li>
<li><a href="#org50ef498"><span class="done CANCELLED">CANCELLED</span> Vector edge features</a></li>
</ul>
</li>
<li><a href="#orge0ea0c3">Future Work</a>
<ul>
<li><a href="#orgda1d9c8">evaluate entire pipeline</a></li>
<li><a href="#orgba80451">Limited testing and experimentation</a></li>
<li><a href="#org8f9464a">Modeling the data as a heterogeneous graph</a></li>
<li><a href="#org21a1dcd">Modeling the problem as a graph classification task</a></li>
<li><a href="#org06f7dc9">Better node and/or edge features</a></li>
<li><a href="#orgbde872f">Advanced scalar edge features</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org4fb6234">References</a></li>
</ul>
</div>
</div>

<div id="outline-container-orgb3f528b" class="outline-2">
<h2 id="orgb3f528b">Inbox</h2>
<div class="outline-text-2" id="text-orgb3f528b">
<ul class="org-ul">
<li><span class="timestamp-wrapper"><span class="timestamp">[2020-08-21 Fri 13:47] </span></span> <a href="https://arxiv.org/abs/1206.5533">paper</a> on practical recommendations for common</li>
</ul>
<p>
hyper parameters for gradient descent based models
</p>
</div>

<div id="outline-container-org67a6e0f" class="outline-3">
<h3 id="org67a6e0f">directions for speedup</h3>
<div class="outline-text-3" id="text-org67a6e0f">
<p>
There are a few that I can think of:
</p>
<ul class="org-ul">
<li>use a faster language: such as C, C++</li>
<li>use a lower abstraction of data structure: such as numpy arrays</li>
<li>use a columnar data store: like parquet</li>
<li>parallelize the pairwise data creation process</li>
<li>distribute the timeslice processing task amongst multiple nodes</li>
</ul>
</div>
</div>

<div id="outline-container-org5659a5b" class="outline-3">
<h3 id="org5659a5b">[paper] recommendations to improve model</h3>
<div class="outline-text-3" id="text-org5659a5b">
<p>
This <a href="https://arxiv.org/abs/1206.5533">paper</a> will be useful to cite as source for the
recommendations I make on improving the km3net models. For a high
level overview, consult Chapter 1: Introduction of the Better Deep
Learning book.
</p>
</div>
</div>
</div>
<div id="outline-container-org8b7eeee" class="outline-2">
<h2 id="org8b7eeee">Pattern Matrix</h2>
<div class="outline-text-2" id="text-org8b7eeee">
<p>
In this section strategies are explored to replace the <i>Hit
Correlation</i> step of the existing data processing pipeline.
</p>
</div>

<div id="outline-container-orgda92555" class="outline-3">
<h3 id="orgda92555">Approach</h3>
<div class="outline-text-3" id="text-orgda92555">
<p>
Two approaches are considered:
</p>
<ol class="org-ol">
<li>Use a MLP to predict if two given points are <i>causally related</i>
or not.</li>
<li>Approach this problem as an unsupervised learning task and use
<b>clustering</b> techniques to determine related points.</li>
</ol>
</div>

<div id="outline-container-org361b5ab" class="outline-4">
<h4 id="org361b5ab">MLP to predict <i>causally related</i> points</h4>
<div class="outline-text-4" id="text-org361b5ab">
<dl class="org-dl">
<dt>causally related</dt><dd>two points are causally related to each
other if they occur close in space and time.</dd>
</dl>

<p>
This approach requires the following sub tasks:
</p>
<ol class="org-ol">
<li><b>Preparation of dataset</b>: Given the "main dataset" a new
dataset (henceforth referred to as the "pattern matrix
dataset") is to be created such that each row contains the
<code>x,y,z and time</code> features for all unique pairs of points.</li>
<li><b>Creation of labels</b>: Using the <i>mc<sub>info</sub></i> table, we can
determine if two points are <i>related</i> if they originated from
the same "event" ie. they have the same <code>event_id</code>.</li>
</ol>
</div>
</div>
</div>

<div id="outline-container-org16a8c61" class="outline-3">
<h3 id="org16a8c61">Creation of "Pattern Matrix" Dataset</h3>
<div class="outline-text-3" id="text-org16a8c61">
<p>
A sample of the <i>main</i> dataset was choosen as input to the "pattern
matrix" dataset creation algorithm. Once generated, further random
samples of varying sizes were considered in order to determine the
optimal training data size. See <a href="#org2ebc555">Experiments</a> for details.
</p>

<p>
Two sampling variants were explored:
</p>
<ol class="org-ol">
<li>random samples from timeslice 615 only</li>
<li>random samples from mixed timeslices: specifically top 5
timeslices with the most number of event hits</li>
</ol>

<p>
The rationale for considering only timeslice 615 being two fold:
</p>
<ol class="org-ol">
<li>It is the timeslice which contains the most number of hits from
neutrino events</li>
<li>and the fact that the model only needs to learn how to identify
"related" and "unrelated" hits, simply done by looking at the
difference of two points. This ofcourse is consistent across
timeslices thus free of any bias.</li>
</ol>
</div>
</div>

<div id="outline-container-orgfb99394" class="outline-3">
<h3 id="orgfb99394">Evaluation</h3>
<div class="outline-text-3" id="text-orgfb99394">
<p>
The <i>main</i> dataset is highly skewed, with the <b>majority</b> class
being hits from background noise and the <b>minority</b> class being
Hits from neutrino events. Thus, the <i>pattern matrix</i> dataset is
also skewed with the <b>minority</b> class being related hits and
<b>majority</b> class being unrelated hits.
</p>

<p>
While the training dataset contains equal number of samples for
each class, the testing dataset maintains it's skewed distribution
since this represents realistic data which the model will be
required to classify.
</p>

<p>
Accuracy is not an ideal metric to use for evaluating the model,
thus the following alternatives are used:
</p>
<ol class="org-ol">
<li>Recall: this should be high indicating the model is able to
identify the minority class</li>
<li>Precision: should ideally be high indicating the model does not
misclassify unrelated hits as related hits, although this is not
a priority (saving a timeslice with no event hits has less
weight compared to <b>not</b> saving a timeslice containing event hits)</li>
<li>F1 score: should be high, however we care more about the recall</li>
<li>F2 score: since we care more about the recall, we give it more
weight while calculating the F-beta score</li>
<li>ROC AUC: although this can be misleading since the ROC considers
both classes and can be over optimistic (due to the skewedness
of data)</li>
<li>Precision-Recall (PR) AUC: a better alternative to the ROC AUC
since it focuses on the minority class</li>
</ol>

<p>
Additionally the ROC curve and the PR curves are also visually
inspected.
</p>

<p>
Relevant sources:
</p>
<ul class="org-ul">
<li><span class="timestamp-wrapper"><span class="timestamp">[2020-08-20 Thu 21:56] </span></span> <a href="https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/">article</a> explaining performance metrics for</li>
</ul>
<p>
imbalanced data
</p>
<ul class="org-ul">
<li><span class="timestamp-wrapper"><span class="timestamp">[2020-08-21 Fri 11:13] </span></span> <a href="https://arxiv.org/pdf/1505.01658.pdf">paper</a> presenting an overview of stratergies
and evaluation techniques for models dealing with highly skewed data</li>
<li><span class="timestamp-wrapper"><span class="timestamp">[2020-08-21 Fri 11:26] </span></span> <a href="https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/">article</a> provides a useful flow chart for
selecting a model evaluation metric when dealing with inbalanced classes</li>
</ul>
</div>

<div id="outline-container-org29aa802" class="outline-4">
<h4 id="org29aa802">Testing datasets</h4>
<div class="outline-text-4" id="text-org29aa802">
<p>
The following variants were picked to test the model:
</p>
<ol class="org-ol">
<li>no related hits (slice 0): no related hits</li>
<li>low related hits (slice 3650): less than 25 related hits</li>
<li>medium related hits (slice 1800): less than 500 related hits</li>
<li>high related hits (slice 1637): less than 1500 related hits</li>
</ol>
</div>
</div>
</div>

<div id="outline-container-org2ebc555" class="outline-3">
<h3 id="org2ebc555"><span class="done DONE">DONE</span> Experiments</h3>
<div class="outline-text-3" id="text-org2ebc555">
<p>
This section provides a summary of all experiments (and their
results) which were conducted in order to obtain the final model to
replace the <i>pattern matrix</i> algorithm of the existing data
processing pipeline. Details of each experiment can be found in the
corresponding notebooks in the <code>notebooks/pm/</code> directory.
</p>

<p>
The final results obtained from each category of experiments are
summarized below. For further details, the corresponding section
for the experiment categories follow.
</p>

<ol class="org-ol">
<li><a href="#org4e1aefd">Experiments with dataset</a>: 10% random sample from mixed
timeslices with equal number of samples for each class produced
the best result.</li>
<li><a href="#orgbfe98cb">Experiments with optimizers</a>: No difference was noticed between
SGD and Adam, <code>lr\=0.001</code> produced the best results across
optimizers.</li>
<li><a href="#org96516a6">Experiments with model architecture</a>: two hidden layers with 16
and 8 neurons respectively produced the best results.</li>
</ol>
</div>

<div id="outline-container-org9e56e08" class="outline-4">
<h4 id="org9e56e08">Notes on selection of epochs</h4>
<div class="outline-text-4" id="text-org9e56e08">
<p>
The number of epochs is varied per experiment. This is
because, this parameter is largely determined by the dataset
itself, and the learning rate of the optimizer.
</p>

<p>
In general, the number of epochs reported in each experiment was
identified by first observing the learning curve and selecting an
ideal value such that the loss was either reasonably minimized or
the validation loss did not deteriorate.
</p>
</div>
</div>
<div id="outline-container-org4e1aefd" class="outline-4">
<h4 id="org4e1aefd"><span class="done DONE">DONE</span> Experiments with dataset</h4>
<div class="outline-text-4" id="text-org4e1aefd">
<p>
In these experiments, variants of the data namely it's shape and
size were manipulated whilst keeping other parameters same. Two
shape variants were considered:
</p>
<ol class="org-ol">
<li><b>original pattern matrix</b> dataset of shape (n, 9)</li>
<li>and <b>diff pattern matrix</b> dataset of shape (n, 5) where the
difference between the (x,y,z,time) features of the points
were taken</li>
</ol>

<p>
Since the dataset is highly skewed, the majority class was
undersampled for each size variant, which are as follows:
</p>
<ol class="org-ol">
<li><b>10%</b> random sample of slice 615</li>
<li><b>25%</b> random sample of slice 615</li>
<li><b>50%</b> random sample of slice 615</li>
<li><b>75%</b> random sample of slice 615</li>
<li><b>10%</b> random sample of slice mixed</li>
</ol>

<p>
Overall, diminishing rewards were observed as the size of the
dataset increased with the <b>mixed-10-equal-diff</b> variant producing the
best results.
</p>
</div>
<ul class="org-ul">
<li><a id="orgf7cf6c7"></a>Summary of results<br />
<div class="outline-text-5" id="text-orgf7cf6c7">
<p>
The following parameters were constant across all experiments:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">parameter</th>
<th scope="col" class="org-left">value</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">loss</td>
<td class="org-left">BCELoss</td>
</tr>

<tr>
<td class="org-left">optimizer</td>
<td class="org-left">SGD with <code>lr\=0.001</code> &amp; <code>momentum\=0.9</code></td>
</tr>

<tr>
<td class="org-left">model architecture</td>
<td class="org-left">(inputs, 10) -&gt; (10,8) -&gt; (8, 1)</td>
</tr>

<tr>
<td class="org-left">activation (hidden)</td>
<td class="org-left">ReLu</td>
</tr>

<tr>
<td class="org-left">activation (output)</td>
<td class="org-left">Sigmoid</td>
</tr>
</tbody>
</table>

<pre class="example">
The slice-mixed-10-equal-diff dataset produced the best results.
</pre>
</div>
</li>
</ul>
</div>

<div id="outline-container-orgbfe98cb" class="outline-4">
<h4 id="orgbfe98cb"><span class="done DONE">DONE</span> Experiments with optimizers</h4>
<div class="outline-text-4" id="text-orgbfe98cb">
<p>
In this class of experiments, different optimizers were used and
their <i>learning rate</i> parameter was varied. This is because
[goodfellow2016deep] suggests that it is the single most important
hyper parameter.
</p>

<p>
The 50%-diff dataset variant was used (since it produced the best
results in the previous class of experiment, see <a href="#org4e1aefd">Experiments with
dataset</a>), all parameters were kept constant whilst <i>lr</i> being
varied to obtain the final model of the category. See
<code>notebooks/pm/exp-optim.ipynb</code> for more details.
</p>

<p>
The different optimizers along with their best results are
summarized below:
</p>
<ol class="org-ol">
<li>SGD: <code>lr\=0.001</code></li>
</ol>

<p>
Overall no improvements were noticed.
</p>
</div>

<ul class="org-ul">
<li><a id="orgf0c6823"></a>Summary of results<br />
<div class="outline-text-5" id="text-orgf0c6823">
<pre class="example">
No discernable difference were noticed between SGD and Adam.
Learning rate of 0.001 gave the best results for both optimizers.
</pre>
</div>
</li>
</ul>
</div>

<div id="outline-container-org96516a6" class="outline-4">
<h4 id="org96516a6"><span class="done DONE">DONE</span> Experiments with model architecture</h4>
<div class="outline-text-4" id="text-org96516a6">
<p>
In this class of experiments the length and breadth of the model
are varied. Multiples of 2 were used to determine the number of
neurons, the minimum being 8 (ie. hidden layer always has a shape
of <code>(8, 1)</code>).
</p>
</div>

<ul class="org-ul">
<li><a id="org5974d47"></a>Summary of results<br />
<div class="outline-text-5" id="text-org5974d47">
<p>
The parameters which gave the best results from the <a href="#org4e1aefd">Experiments
with dataset</a> class of experiments were chosen whilst varying the
length and depth of the model.
</p>

<pre class="example">
The best results were obtained by setting the model architecture
as =(inputs, 16) -&gt; (16, 8) -&gt; (8,1)= with a recall of 0.81. The
results were deemed good enough for this model and thus
experiments for the PM model were concluded.
</pre>
</div>
</li>
</ul>
</div>
</div>

<div id="outline-container-orgfea82ee" class="outline-3">
<h3 id="orgfea82ee">Future Work</h3>
<div class="outline-text-3" id="text-orgfea82ee">
<p>
Improvements that can be made to the MLP model.
</p>

<p>
The MLP can be replaced with a GNN to perform edge type
classification. The Pytorch Geometric has an implementation of the
EdgeConv network.
</p>
</div>

<div id="outline-container-org17073ee" class="outline-4">
<h4 id="org17073ee">Advanced relations</h4>
<div class="outline-text-4" id="text-org17073ee">
<p>
The PM model is naive as it only treats event-event pairs from the
same event as related and the rest as unrelated. This is too
simplistic since noise-noise pairs are also related and
event-event pairs from different events are related (since they
are both event hits) but perhaps to a lesser degree than pairs
from the same event.
</p>

<p>
We can make the PM model a 4 class classifier such that it
classifying the pair type (event-event-same, event-event-different
event-noise and noise-noise). This can be further utilized to
assign varying weights or edge-types to the edges of the graph.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgdad1005" class="outline-2">
<h2 id="orgdad1005">Graph Community Detection</h2>
<div class="outline-text-2" id="text-orgdad1005">
<p>
This section describes the strategies explored to replace the <i>Graph
Community Detection</i> step for the existing data processing pipeline.
</p>
</div>

<div id="outline-container-orgd1ee1f7" class="outline-3">
<h3 id="orgd1ee1f7">Edge weights in GNNs</h3>
<div class="outline-text-3" id="text-orgd1ee1f7">
<p>
According to the Deep Graph Library, the edge weights are
incorporated differently in models. The general method is to scale
the node embedding propagating through it.
</p>

<p>
This means, with the naive edge weight scheme, unrelated nodes do
not exchange embeddings (embedding gets multiplied by 0). This
caused two questions in my mind:
</p>

<ul class="org-ul">
<li>does the advanced edge weights work simply because all nodes are
now exchanging their node embeddings?</li>
<li>and second, if unrelated nodes do not exchange their embeddings
with naive edge weights, then how are their communities in the
training set?</li>
</ul>

<p>
It is possible that the network implementation I am using is not
scaling the embeddings by the edge weight. I tried to dig into the
<code>GCNConv</code> implementation that pytorch geometric is using but could
not decipher anything.
</p>

<p>
In hindsight, I suppose I should have experimented with the
expected probability of positive class as edge weights before
jumping to advanced weights. Generating the probabilities
synthetically would have been hard and the only difference with
advanced weight scheme is that noise-noise pairs are also assigned
a high edge weight. 
</p>
</div>
</div>

<div id="outline-container-org0a426f9" class="outline-3">
<h3 id="org0a426f9"><span class="done CANCELLED">CANCELLED</span> Why we do not need graphConv</h3>
<div class="outline-text-3" id="text-org0a426f9">
<p>
Or, why the graph approach is a different research direction
altogether.
Our end goal is : <b>Given a timeslice, should I save it?</b>
If we put 100% faith and trust in the simulated data, and we train
a neural network to identify hits which are related to each other
(ie. they originated from the same event). Then, presence of
related hits above a certain threshold (say 10) directly implies
that the timeslice is worth saving.
</p>

<p>
Graph neural networks on the other hand work on different
principles. Most relevant to this project would be to do node
classification which is a semi supervised form of learning. Given
labels for some of the nodes, the network can predict labels for
the rest.
</p>

<p>
Given that a MLP is much simpler, it should be the preferred over
Graph Networks.
</p>
</div>
</div>
<div id="outline-container-orge603ac8" class="outline-3">
<h3 id="orge603ac8">Primer on Graph Neural Networks</h3>
<div class="outline-text-3" id="text-orge603ac8">
<p>
It is important to understand the different applications of Graph
Neural Networks (GNNs) before we proceed. GNNs have two primary
applications:
</p>
<ol class="org-ol">
<li><b>Node classification</b> which is a semi-supervised learning
setting. The idea is that given a graph with partial labels, we
want to conduct label propagation.</li>
<li><b>Graph classification</b> which is a supervised learning setting.
Here we have several graphs with a corresponding label and we
want to classify them.</li>
</ol>

<p>
In this project the data is modeled to facilitate <b>node classification</b>.
</p>
</div>
</div>

<div id="outline-container-org860840f" class="outline-3">
<h3 id="org860840f">Approach</h3>
<div class="outline-text-3" id="text-org860840f">
<p>
Let's say we have data for timeslices in the form of <code>(n, 5)</code>
dataframe (5 features because <code>x, y, z, t and label</code>). Each row,
then can be represented as a node with <code>(x,y,z,t)</code> as its feature
vector with a corresponding <code>label</code> to indicate if it is a event or
a noise node.
</p>

<p>
We can experiment with the edge attributes of the graph:
</p>
<ol class="org-ol">
<li>No edge attributes: ie. only node features are provided</li>
<li>Scalar edge attributes of shape <code>(num_edges,)</code> ie. each edge has
a 1d weight between [0, 1], obtained from the PM model</li>
<li>Vector edge attributes of shape <code>(num_edges, m)</code> ie. each edge
has column vector of shape <code>(m,)</code> as the weight.</li>
</ol>

<p>
Results and conclusions from each experiment can be found in the
<a href="#org2ebc555">Experiments</a> section below.
</p>
</div>
</div>

<div id="outline-container-org0038ecc" class="outline-3">
<h3 id="org0038ecc">Data Preparation</h3>
<div class="outline-text-3" id="text-org0038ecc">
<p>
The training data consists of a <code>(n, 4)</code> node feature matrix, <code>(n,
   )</code> column vector of node labels and <code>(num_edges,)</code> column vector of
edge weights. The node feature matrix and labels are easily
obtained from the main dataset. The edge weights can be obtained
from the output of <code>km3net.data.model.process()</code> and selecting the
label column.
</p>

<p>
Note that pytorch geometric requires that <code>num_edges</code> be of length
<code>n^{2} - n</code>. To facilitate this, <code>km3net.data.model.process()</code>
accepts a <code>model</code> parameter which should be set to 'gcd'.
</p>
</div>
</div>

<div id="outline-container-orgd19cb7a" class="outline-3">
<h3 id="orgd19cb7a">Evaluation</h3>
<div class="outline-text-3" id="text-orgd19cb7a">
<p>
Evaluation is done using the same datasets and metrics used for the
PM model (see <a href="#orgfb99394">Evaluation</a>).
</p>
</div>
</div>

<div id="outline-container-orgb361d64" class="outline-3">
<h3 id="orgb361d64">Experiments</h3>
<div class="outline-text-3" id="text-orgb361d64">
<p>
To summarize, training the model with edge weights produced the
best results with a high precision and recall for <b>both</b> classes
(ie. false positives and negatives are low).
</p>

<p>
Assigning a <code>(n,)</code> column vector instead of a scaler as edge features was
deemed beyond the scope of this project. This requires the data to
be modeled as a heterogeneous graph with 'n' types of edges each
carrying the corresponding scalar weight.
</p>

<p>
The following parameters were kept constant across all experiments:
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">parameter</th>
<th scope="col" class="org-left">value</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">loss</td>
<td class="org-left">BCELoss</td>
</tr>

<tr>
<td class="org-left">optimizer</td>
<td class="org-left">Adam with <code>lr\=0.01</code></td>
</tr>

<tr>
<td class="org-left">model architecture</td>
<td class="org-left">(inputs, 16) -&gt; (16,2) -&gt; (2, 1)</td>
</tr>

<tr>
<td class="org-left">activation (hidden)</td>
<td class="org-left">ReLu</td>
</tr>

<tr>
<td class="org-left">activation (output)</td>
<td class="org-left">Sigmoid</td>
</tr>
</tbody>
</table>

<p>
A Graph convolusional layer was used for Layer 1 and 2 whilst a
Linear layer was used as the final layer to perform the final
classification.
</p>

<p>
Details for each experiment follow.
</p>
</div>

<div id="outline-container-org693718b" class="outline-4">
<h4 id="org693718b">No edge features</h4>
<div class="outline-text-4" id="text-org693718b">
<p>
Model was trained using no edge features for varying sizes of
training data.
</p>

<pre class="example">
Resulted in a no skill classifier.
</pre>
</div>
</div>

<div id="outline-container-org57edca1" class="outline-4">
<h4 id="org57edca1">Scalar edge features</h4>
<div class="outline-text-4" id="text-org57edca1">
<p>
Weights were assigned to the edges. A weight of 1 was assigned to
nodes which are 'causally related' (see <a href="#org361b5ab">MLP to predict <i>causally
related</i> points</a>) whilst a weight of 0 was assigned to others (this
is mimicking the output of the PM model).
</p>

<p>
Experiments were conducted with varying model architecture,
learning rate, weight decay and probability threshold.
</p>

<pre class="example">
The model is highly biased to the positive class, the probability
threshold is set to 0.99 to obtain a decent model with an accuracy
of 0.7
</pre>
</div>

<ul class="org-ul">
<li><a id="org2c1770c"></a>[exp,gcd] advanced weights<br />
<div class="outline-text-5" id="text-org2c1770c">
<p>
<b>Motivation</b> Model is highly skewed to positive class when trained
with naive weights. Probability threshold manipulation produces a
brittle model (the threshold changes with the shape and size of the
training set).
</p>

<p>
<b>Hypothesis</b> Instead, we assign weights to edges based on the type
of node:
</p>
<ul class="org-ul">
<li>noise-noise pair: 1.0</li>
<li>event-event (same event id) pair: 1.0</li>
<li>event-event (different event id) pair: 0.5</li>
<li>event-noise pair: 0.1</li>
</ul>

<p>
<b>Result</b> With the advanced weights the network has no false
positives and false negatives in test timeslices which contained
event nodes. However, in slice 0 (no event hits) it thought
everything was a event hit (ie. all predicted labels were false
positives).
</p>

<p>
The model thus is biased by the presence of the high weight on the
edge of noise-noise pairs since a similar weight exists between
event-event (same event id) pairs as well. So, the model is unable
to learn anything meaningful from the node features.
</p>

<p>
<b>Next</b> Assign a low weight to noise-noise pairs.
</p>

<p>
<b>next</b> put no weights on noise-noise pairs
</p>
</div>
</li>

<li><a id="orgde44b14"></a>[exp,gcd] low weights for noise-noise edges<br />
<div class="outline-text-5" id="text-orgde44b14">
<p>
<b>Motivation</b> high false positives in model trained with advanced
weights when tested with timeslice with no event nodes.
</p>

<p>
<b>Hypothesis</b> model classifies noise nodes in timeslice with no
event nodes as well.
</p>

<p>
<b>Result</b> This results in a no skill classifier. This makes sense
because we also assign a low weight of 0.1 to noise-event pairs.
Now with noise-noise pairs with the same edge weights, the model is
no longer able to identify the noise nodes properly.
</p>

<p>
The expectation was that the model learns something from the node
features but this does not seem to be the case.
</p>

<p>
<b>Next</b> Any further improvements to the model is beyond the scope of
 this project. Propose all the improvements and alternative paths
 of research in the report. This concludes the experimentation and
 practical side of the thesis!
</p>
</div>
</li>
</ul>
</div>
<div id="outline-container-org50ef498" class="outline-4">
<h4 id="org50ef498"><span class="done CANCELLED">CANCELLED</span> Vector edge features</h4>
<div class="outline-text-4" id="text-org50ef498">
<p>
This is possible to do however was deemed beyond the scope of this
project see <a href="#org2ebc555">Experiments</a> for reasoning.
</p>
</div>
</div>
</div>

<div id="outline-container-orge0ea0c3" class="outline-3">
<h3 id="orge0ea0c3">Future Work</h3>
<div class="outline-text-3" id="text-orge0ea0c3">
<p>
Several alternative paths of research are touched upon in this section.
</p>
</div>
<div id="outline-container-orgda1d9c8" class="outline-4">
<h4 id="orgda1d9c8">evaluate entire pipeline</h4>
<div class="outline-text-4" id="text-orgda1d9c8">
<p>
Evaluate the entire pipeline using the best performing models with
various timeslices and observe how well the pipeline is able to
identify timeslices worth saving.
</p>

<p>
Since this requires modifying the PM model to perform multi-label
classification, suggest this as future work.
</p>
</div>
</div>

<div id="outline-container-orgba80451" class="outline-4">
<h4 id="orgba80451">Limited testing and experimentation</h4>
<div class="outline-text-4" id="text-orgba80451">
<p>
Due to pytorch being built from source and the viltstift not
having Nvidia GPUs, pytorch geometric could not be installed as it
cannot run on AMD hardware.
</p>

<p>
As a result, experimentation had to be done on Google Colab which
puts restrictions on disk space and memory. Due to these
limitations, models could only be trained on graphs with a max on
1000 nodes and tested with graphs of similar size.
</p>
</div>
</div>

<div id="outline-container-org8f9464a" class="outline-4">
<h4 id="org8f9464a">Modeling the data as a heterogeneous graph</h4>
<div class="outline-text-4" id="text-org8f9464a">
<p>
To accommodate <a href="#org50ef498">Vector edge features</a> experiment.
</p>
</div>
</div>

<div id="outline-container-org21a1dcd" class="outline-4">
<h4 id="org21a1dcd">Modeling the problem as a graph classification task</h4>
<div class="outline-text-4" id="text-org21a1dcd">
<p>
We can construct a graph from a given timeslice and train a model
to classify it as <code>SAVE</code> or <code>NOSAVE</code> based on presence of events.
At the time of writing this, the best approach to create the
dataset is unclear.
</p>

<p>
Here, each timeslice is a graph where each hit (row of dataframe)
is represented by a node and all nodes are connected by undirected
edges. It's label can be obtained by looking at the number of
event hits present and setting it to 1 if the count is above a
certain threshold. Each node of the graph has an embedding/feature
vector corresponding to the <code>(4,)</code> feature vector (row of the
dataframe).
</p>
</div>
</div>

<div id="outline-container-org06f7dc9" class="outline-4">
<h4 id="org06f7dc9">Better node and/or edge features</h4>
<div class="outline-text-4" id="text-org06f7dc9">
<p>
The node feature and the edge features can be improved such that
it is more meaningful to the model.
</p>
</div>
</div>

<div id="outline-container-orgbde872f" class="outline-4">
<h4 id="orgbde872f">Advanced scalar edge features</h4>
<div class="outline-text-4" id="text-orgbde872f">
<p>
The current edge features are simplistic as a really high weight
(of 1) if assigned to event nodes from the same event thus all
other edge types 1. (event-event diff event id) 2. event-noise
and 3. noise-noise edges have a weight of 0.
</p>

<p>
Edges can instead be given a weight based on the type of edge,
with the hopes that it helps the network classify the nodes better.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org4fb6234" class="outline-2">
<h2 id="org4fb6234">References</h2>
<div class="outline-text-2" id="text-org4fb6234">
<dl class="org-dl">
<dt>[goodfellow2016deep]</dt><dd>Goodfellow, I., Bengio, Y., Courville, A., &amp;
Bengio, Y. (2016). Deep learning (Vol. 1). Cambridge: MIT press.</dd>
</dl>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: August 20, 2020</p>
<p class="author">Author: Arumoy Shome</p>
<p class="date">Created: 2021-01-30 Sat 20:45</p>
</div>
</body>
</html>
